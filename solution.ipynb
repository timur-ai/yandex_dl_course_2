{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a4557d",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Text Autocompletion Project — From LSTM to Transformers\n",
    "\n",
    "This notebook-style Python script implements the complete pipeline required by the\n",
    "project brief. It is structured as a sequence of Markdown explanations followed by\n",
    "executable cells. The scope and goals mirror the assignment:\n",
    "\n",
    "- Prepare and normalize a corpus of short posts from a line-delimited text file.\n",
    "- Build and train a compact next-token LSTM language model suitable for constrained\n",
    "  devices.\n",
    "- Measure quality using ROUGE-1 and ROUGE-2 on held-out text completions (predicting\n",
    "  the last quarter of each text from the first three quarters).\n",
    "- Benchmark against a pretrained transformer baseline (DistilGPT2) using a simple\n",
    "  generation pipeline.\n",
    "- Compare results and provide a concise conclusion.\n",
    "\n",
    "The machine learning formulation is next-token prediction: given tokens\n",
    "$(w_1, \\dots, w_n)$, the model estimates $P(w_{n+1} \\mid w_{\\le n})$ and we\n",
    "iteratively generate until an end token or a maximum length is reached.\n",
    "\n",
    "Datasets are split deterministically into train/validation/test, tokenized at the\n",
    "word level with special tokens `<pad>`, `<unk>`, `<bos>`, `<eos>`, and converted into\n",
    "shifted input/target tensors for supervised learning.\n",
    "\n",
    "Evaluation follows the assignment’s rule of using the first 75% tokens of each text as\n",
    "the prefix and comparing generated completions to the remaining 25% via ROUGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ede38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config_utils import LSTMConfig, load_config_model\n",
    "from src.data_utils import load_processed_texts, preprocess_dataset, split_and_persist_texts\n",
    "from src.eval_lstm import evaluate_on_texts\n",
    "from src.eval_transformer_pipeline import evaluate_gpt2_on_texts\n",
    "from src.lstm_model import LSTMLanguageModel\n",
    "from src.lstm_train import create_optimizer_scheduler, save_checkpoint, train_epoch, valid_epoch\n",
    "from src.next_token_dataset import create_next_token_dataloaders\n",
    "from src.project_utils import initialize_runtime, print_environment\n",
    "from src.summary import print_comparison, print_scores, show_examples\n",
    "from src.tokenizer_vocab import WordVocab, build_supervised_tensors\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9c7de",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Configuration\n",
    "\n",
    "We load a typed configuration object that centralizes paths, data limits, model\n",
    "hyperparameters, training settings, and generation defaults. This ensures the entire\n",
    "experiment is reproducible and easy to tweak without touching the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360629d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = load_config_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f20fe1",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Runtime Initialization\n",
    "\n",
    "We set a fixed seed for reproducibility, select the device, optionally limit CPU\n",
    "threads, and print the environment. Keeping runs deterministic is crucial for fair\n",
    "comparisons between the LSTM and the transformer baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f076f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = initialize_runtime(seed=42, limit_cpu_threads=True)\n",
    "device = ctx.device\n",
    "print_environment(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df322c",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Data Preparation (Normalization and Filtering)\n",
    "\n",
    "We ingest a line-delimited UTF-8 text file (`tweets.txt`, one text per line) and produce a single-column `text` CSV after:\n",
    "- Lowercasing and basic normalization\n",
    "- Removing URLs, mentions, and optional emojis\n",
    "- Collapsing excessive whitespace\n",
    "- Filtering by minimal character count and maximal token length\n",
    "\n",
    "This creates a clean, consistent corpus for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_csv = preprocess_dataset(\n",
    "    raw_data_path=CFG.data.raw_data_path,\n",
    "    out_path=CFG.data.processed_path,\n",
    "    dev_limit=CFG.data.dev_limit,\n",
    "    min_chars=CFG.preprocessing.min_chars,\n",
    "    max_tokens=CFG.preprocessing.max_tokens,\n",
    ")\n",
    "print(f\"Processed dataset written to: {processed_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355b493",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Load Processed Texts\n",
    "\n",
    "We read the normalized single-column corpus. For fast iteration, we may limit the\n",
    "number of examples during development via the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaeb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_processed_texts(CFG.data.processed_path, limit=CFG.data.dev_limit)\n",
    "print(f\"Loaded {len(texts):,} texts from {CFG.data.processed_path}\")\n",
    "print(\"Sample:\", texts[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddaff12",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Deterministic Split: Train / Validation / Test\n",
    "\n",
    "We perform a seeded shuffle and split into train, validation, and test subsets, then\n",
    "persist each split to disk. This mirrors how production data pipelines audit and\n",
    "version data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cfe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, test_texts, _paths = split_and_persist_texts(\n",
    "    texts=texts,\n",
    "    train_path=CFG.data.train_path,\n",
    "    val_path=CFG.data.val_path,\n",
    "    test_path=CFG.data.test_path,\n",
    "    train_frac=CFG.split.train_frac,\n",
    "    val_frac=CFG.split.val_frac,\n",
    "    seed=CFG.split.seed,\n",
    "    dev_limit=CFG.data.dev_limit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573931b",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Tokenizer and Vocabulary\n",
    "\n",
    "We build a compact word-level vocabulary with special tokens `<pad>`, `<unk>`, `<bos>`,\n",
    "and `<eos>`. The tokenizer uses a configurable token pattern and frequency threshold to\n",
    "control vocabulary size, trading off coverage and memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = WordVocab.build(\n",
    "    texts,\n",
    "    special_tokens={\n",
    "        \"pad\": CFG.special_tokens.pad,\n",
    "        \"unk\": CFG.special_tokens.unk,\n",
    "        \"bos\": CFG.special_tokens.bos,\n",
    "        \"eos\": CFG.special_tokens.eos,\n",
    "    },\n",
    "    max_vocab_size=CFG.tokenizer.max_vocab_size,\n",
    "    token_pattern=CFG.tokenizer.token_pattern,\n",
    "    min_freq=CFG.tokenizer.min_freq,\n",
    ")\n",
    "print(\"Vocab size (including specials):\", len(vocab.id_to_token))\n",
    "print(\"Special IDs:\", {k: getattr(vocab, f\"{k}_id\") for k in (\"pad\", \"unk\", \"bos\", \"eos\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85086a6",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Supervised Tensors for Next-Token Prediction\n",
    "\n",
    "We convert each text into three tensors per split:\n",
    "- `input_ids`: token IDs including `<bos>` and `<eos>`, padded to a fixed length\n",
    "- `target_ids`: `input_ids` shifted left by one position\n",
    "- `attention_mask`: marks valid (non-pad) positions for loss masking\n",
    "\n",
    "This enables standard cross-entropy training for next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f423dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_target_ids, train_attention_mask = build_supervised_tensors(\n",
    "    train_texts, vocab, max_len=CFG.sequence.max_length, device=device\n",
    ")\n",
    "val_input_ids, val_target_ids, val_attention_mask = build_supervised_tensors(\n",
    "    val_texts, vocab, max_len=CFG.sequence.max_length, device=device\n",
    ")\n",
    "test_input_ids, test_target_ids, test_attention_mask = build_supervised_tensors(\n",
    "    test_texts, vocab, max_len=CFG.sequence.max_length, device=device\n",
    ")\n",
    "\n",
    "print(\"Splits:\", len(train_texts), len(val_texts), len(test_texts))\n",
    "print(\"Train tensors:\", train_input_ids.shape, train_target_ids.shape, train_attention_mask.shape)\n",
    "print(\"Val tensors:\", val_input_ids.shape, val_target_ids.shape, val_attention_mask.shape)\n",
    "print(\"Test tensors:\", test_input_ids.shape, test_target_ids.shape, test_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b48b7",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## DataLoaders\n",
    "\n",
    "We wrap the tensors into `DataLoader`s for mini-batch training and evaluation, shuffling\n",
    "only the training split. This keeps validation/test metrics comparable across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_next_token_dataloaders(\n",
    "    train_tensors=(train_input_ids, train_target_ids, train_attention_mask),\n",
    "    val_tensors=(val_input_ids, val_target_ids, val_attention_mask),\n",
    "    test_tensors=(test_input_ids, test_target_ids, test_attention_mask),\n",
    "    cfg=CFG,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f1323",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## LSTM Language Model\n",
    "\n",
    "We instantiate a compact LSTM-based language model configured for mobile-friendly\n",
    "footprint. The model predicts the next token distribution at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LSTMConfig(\n",
    "    num_embeddings=len(vocab.id_to_token),\n",
    "    embedding_dim=CFG.model.lstm.embedding_dim,\n",
    "    hidden_size=CFG.model.lstm.hidden_size,\n",
    "    num_layers=CFG.model.lstm.num_layers,\n",
    "    dropout=CFG.model.lstm.dropout,\n",
    "    pad_token_id=vocab.pad_id,\n",
    ")\n",
    "model = LSTMLanguageModel(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c6cac",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "### Parameter Count\n",
    "\n",
    "We report total and trainable parameters to gauge capacity and memory cost. This helps\n",
    "justify design choices when targeting limited-resource environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2409fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model params: total={num_params:,}, trainable={num_params_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d201f82",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Optimization Setup\n",
    "\n",
    "We use standard cross-entropy training with an optimizer configured from the config and\n",
    "an optional StepLR scheduler. Gradient clipping is applied during the training step to\n",
    "stabilize optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, scheduler = create_optimizer_scheduler(\n",
    "    model,\n",
    "    learning_rate=CFG.training.learning_rate,\n",
    "    weight_decay=CFG.training.weight_decay,\n",
    "    use_scheduler=CFG.scheduler.use,\n",
    "    step_size=CFG.scheduler.step_size,\n",
    "    gamma=CFG.scheduler.gamma,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc1667",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Training Loop and Loss Curves\n",
    "\n",
    "We train for a small number of epochs, recording train/validation loss per epoch. A\n",
    "loss curve is saved to `data/loss_curve.png` for quick inspection of convergence and\n",
    "generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "train_losses: list[float] = []\n",
    "val_losses: list[float] = []\n",
    "for epoch in range(1, (CFG.training.epochs or 1) + 1):\n",
    "    tr = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        max_grad_norm=CFG.training.max_grad_norm,\n",
    "        progress_desc=f\"epoch {epoch}/{CFG.training.epochs or 1}\",\n",
    "    )\n",
    "    va = valid_epoch(model, val_loader, device)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    train_losses.append(tr)\n",
    "    val_losses.append(va)\n",
    "    print(f\"epoch {epoch}: train={tr:.4f} val={va:.4f}\")\n",
    "\n",
    "if len(train_losses) > 0:\n",
    "    epochs_axis = list(range(1, len(train_losses) + 1))\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(epochs_axis, train_losses, label=\"Train loss\")\n",
    "    plt.plot(epochs_axis, val_losses, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training/Validation Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c052b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Checkpointing\n",
    "\n",
    "We save a minimal checkpoint containing the model parameters and the vocabulary. This\n",
    "is sufficient to run inference and evaluation later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42973faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model, vocab, state_path=str(Path(\"data\") / \"lstm_state.pt\"), vocab_path=str(Path(\"data\") / \"vocab.pt\"))\n",
    "print(\"Saved model state and vocab to data/.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d10cf",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Evaluation: LSTM on Test Split\n",
    "\n",
    "Using the assignment protocol, we evaluate ROUGE-1/2 (precision, recall, F1) by\n",
    "generating the last quarter of each text from its first three quarters and comparing to\n",
    "the reference continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = evaluate_on_texts(\n",
    "    model,\n",
    "    test_texts,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    max_new_tokens=CFG.generation.max_new_tokens,\n",
    "    max_samples=CFG.evaluation.test_samples,\n",
    ")\n",
    "print(\"LSTM test ROUGE:\", {k: round(v, 4) for k, v in test_scores.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983a62b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Evaluation: DistilGPT2 Baseline\n",
    "\n",
    "We benchmark a pretrained `distilgpt2` via the transformers text-generation pipeline on\n",
    "the same protocol to contextualize the LSTM’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_test_scores = evaluate_gpt2_on_texts(\n",
    "    texts=test_texts,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    model_name=CFG.gpt2.model_name,\n",
    "    max_new_tokens=CFG.generation.max_new_tokens,\n",
    "    max_samples=CFG.gpt2.eval_samples,\n",
    "    token_pattern=CFG.tokenizer.token_pattern,\n",
    ")\n",
    "print(\"GPT2 Test ROUGE:\", {k: round(v, 4) for k, v in gpt2_test_scores.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dddb36",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary and Qualitative Examples\n",
    "\n",
    "We print ROUGE summaries for both models, provide a side-by-side comparison, and show\n",
    "qualitative completions for a handful of train/validation/test samples to build\n",
    "intuition beyond aggregate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c411e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(\"LSTM Test ROUGE (P/R/F1):\", test_scores)\n",
    "print_scores(\"DistilGPT2 Test ROUGE (P/R/F1):\", gpt2_test_scores)\n",
    "print_comparison(\"LSTM (Test)\", test_scores, \"DistilGPT2 (Test)\", gpt2_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17104d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_examples(model, train_texts, vocab, device, CFG.generation.max_new_tokens, k=3)\n",
    "show_examples(model, val_texts, vocab, device, CFG.generation.max_new_tokens, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84409c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_examples(model, test_texts, vocab, device, CFG.generation.max_new_tokens, k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ee398",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We select the better model by ROUGE-1 F1 on the test split and print the decision. In\n",
    "practice, the transformer baseline often wins on quality, while the LSTM may be\n",
    "preferable under strict memory and latency constraints. The final choice depends on the\n",
    "target device budget and user experience requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad643e30",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "gpt2_r1 = gpt2_test_scores.get(\"rouge1_f1\")\n",
    "lstm_r1 = test_scores.get(\"rouge1_f1\")\n",
    "gpt2_test = float(gpt2_r1) if isinstance(gpt2_r1, int | float) and gpt2_r1 == gpt2_r1 else float(\"nan\")\n",
    "lstm_test = float(lstm_r1) if isinstance(lstm_r1, int | float) and lstm_r1 == lstm_r1 else float(\"nan\")\n",
    "better = \"DistilGPT2\" if gpt2_test > lstm_test else \"LSTM\"\n",
    "print(f\"Conclusion: {better} performed better on ROUGE-1 F1 on the test split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc441a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
